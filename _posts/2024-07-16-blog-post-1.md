---
title: 'Single-Image Personalization via Instance-Specific Fine-tuning of Diffusion Models'
date: 2024-07-16
permalink: /posts/2024/07/blog-post-1/
tags:
  - Diffusion Models
  - Instance-Specific Fine-tuning
  - Generative AI
---

This work addresses the challenge of instance-specific fine-tuning of diffusion models using only a single reference image - a significant constraint compared to existing approaches that require multiple instance images. We present a novel two-stage optimization framework that enables high-fidelity instance integration while preserving model generalization. The method leverages careful regularization through prior-preservation losses and strategic hyperparameter optimization to achieve effective personalization without overfitting.

## Background: Diffusion Models

Our approach builds on Denoising Diffusion Probabilistic Models (DDPMs), which learn to reverse a gradual noising process. The forward process applies Gaussian noise over timesteps t according to:

$$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I) $$

where \\(\beta_t\\) controls the noise schedule. The reverse process learns to denoise through:

$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

with learned parameters \\(\mu_\theta\\) and \\(\Sigma_\theta\\) predicting the mean and variance of each denoising step.

## Single-Image Personalization

While existing approaches like DreamBooth require 3-5 instance images for effective fine-tuning, we demonstrate that careful optimization can achieve comparable results with a single reference image. Our method binds unique identifier tokens to both target scenes and instance subjects, enabling controlled generation through text prompts.

The key challenge lies in preventing overfitting while capturing sufficient instance-specific details. Initial experiments revealed that precise learning rate scheduling and step count optimization are crucial for maintaining this balance.

## Methodology

Our approach consists of two primary stages:

### Stage I: Target Scene Optimization

The first stage fine-tunes the model on the target scene using a combination of instance-specific and prior-preservation losses:

$$ L_{\text{target}} = \mathbb{E}_{(x,c),\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta(z_t, t, \tau_\theta(c)) \|_2^2 \right] + \lambda \mathbb{E}_{(x_{\text{pr}}, c_{\text{pr}}),\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta(z_{\text{pr}}, t, \tau_\theta(c_{\text{pr}})) \|_2^2 \right] $$

where \\(\epsilon\\) represents the noise vector, \\(z_t\\) is the latent encoding at timestep t, and \\(\tau_\theta(c)\\) and \\(\tau_\theta(c_{\text{pr}})\\) encode instance-specific and general content respectively.

### Stage II: Instance Integration

The second stage optimizes for instance-specific token binding while preserving the scene composition capabilities developed in Stage I. The loss function maintains the same structure but focuses on instance-specific features.

![Method](/images/method.png)

## Experimental Validation

We evaluated our method using a diverse dataset of eight image pairs, comprising various target scenes and instance types. Each pair represents a distinct combination of scene complexity and instance characteristics, allowing comprehensive assessment of the method's generalization capabilities.

### Results and Analysis

![Generated Images](/images/results.png)

Quantitative evaluation employed two primary metrics:
1. Learned Perceptual Image Patch Similarity (LPIPS) for assessing perceptual quality
2. CLIP-T scores for measuring semantic consistency

Results demonstrate effective instance integration across various scene-instance combinations, with particularly strong performance on inanimate objects and structured scenes. Complex instances like human faces exhibited higher sensitivity to hyperparameter selection.

### Hyperparameter Optimization

Critical hyperparameters include:
- Learning rate schedules for both stages
- Training step counts
- Prior-preservation loss weights

Optimal settings varied based on instance complexity, with inanimate objects generally permitting more aggressive optimization compared to organic subjects.

## Implementation Details

All experiments were conducted on a single A6000 GPU. The implementation leverages custom training loops for fine-grained control over the optimization process. Model architectures and base weights remain consistent across experiments, with variations only in training hyperparameters.

## Limitations and Future Work

While the current approach demonstrates promising results, several limitations warrant further investigation:
1. Limited instance diversity due to single-image constraints
2. Imperfect fidelity in complex instance-scene combinations
3. Sensitivity to hyperparameter selection

Future work will explore the SINE framework's patch-based fine-tuning approach, which decouples pixel positions from semantic concepts. Additionally, investigating alternative regularization strategies and adaptive hyperparameter selection methods may further improve robustness.

## Conclusion

This work demonstrates the feasibility of single-image personalization in diffusion models through careful optimization and regularization. While challenges remain in achieving perfect fidelity for complex instances, the proposed method significantly reduces the data requirements for personalized image generation while maintaining generation quality.

For implementation details and full experimental results, please refer to the complete [technical report](/files/CS_6384_Project_Final_Report.pdf).